# Scripts Summary: Data Pipeline and Evaluation Infrastructure

The `scripts/` directory implements a complete data acquisition, processing, and evaluation pipeline for the World Mind POC-4 Rivers Extended experiment. The pipeline extracts river knowledge from DBpedia's structured knowledge graph, enriches it with LLM-augmented metadata, generates evaluation questions, and measures baseline hallucination rates across multiple model architectures.

**Data Acquisition:** `get_data.py` queries DBpedia's SPARQL endpoint to retrieve US river entities with 21 structured attributes including hydrological metrics (length, discharge, watershed areas), geographical coordinates, ecological relationships, and administrative jurisdictions. The SPARQL query filters rivers by US state classifications, returning approximately 9,538 unique river entities with abstracts and metadata that form the foundational knowledge base for downstream evaluation.

**Knowledge Augmentation:** `fill_missing_data.py` employs LLM-based information extraction to complete sparse attribute fields in the raw dataset. Using Gemini 2.5 Flash Lite via OpenRouter API, it infers missing values from natural language abstracts (e.g., extracting source elevations, tributary relationships, alternative names) and applies deterministic regex-based extraction for variant names. This augmentation creates an enhanced dataset (`raw_rivers_filled.csv`) that maximizes the referential surface area for testing temporal and spatial consistency constraints.

**Question Generation:** `generate_qa_dataset.py` synthesizes 17,726 multiple-choice questions by prompting LLMs to create factually-grounded quiz questions from river abstracts. Each river produces two questions with five answer choices, where one answer is explicitly verifiable from the abstract and four are plausible distractors. The script enforces strict quality controls: questions must be self-contained (no meta-references to "abstract" or "provided information"), test specific factual details, and use similar-value incorrect options to prevent trivial elimination strategies.

**Model Evaluation:** `evaluate_llms.py` implements a resumable, production-grade assessment pipeline that tests arbitrary LLM models via OpenRouter API on the Q&A dataset. It handles incremental processing with state persistence, extracts single-character multiple-choice responses (A-E), validates correctness against ground truth, and serializes results with timestamps for failure analysis. The evaluator supports rate limiting, error recovery, and concurrent model testing, enabling systematic comparison of hallucination patterns across different architectural scales and training paradigms.

**Comparative Analysis:** `compare_models.py` aggregates evaluation summaries from multiple completed runs, sorting models by accuracy to identify performance hierarchies and extract insights about knowledge boundary patterns. This utility supports the empirical analysis required to demonstrate that graph-licensed abstention provides measurable improvements over baseline probabilistic generation across diverse model architectures, directly supporting the research contribution's claims about architectural superiority over prompt engineering alone.

