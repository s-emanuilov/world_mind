# Fine-Tuning Summary: Instruction-Tuned Abstention Models

The `fine-tuning/` directory implements supervised fine-tuning of compact LLMs to learn principled abstention behavior—a critical architectural capability explored in the World Mind research. Two parallel training datasets (`dataset_with_all_factual_data.jsonl` and `dataset_with_abstrain.jsonl`) were constructed from the 17,726 river Q&A pairs, where correct answers populate the factual dataset while incorrect answers are replaced with "I don't know" in the abstention dataset. This creates a training signal that encourages models to either generate factual responses or explicitly defer when knowledge is absent.

The `train.ipynb` notebook orchestrates the complete fine-tuning pipeline using Unsloth AI for efficient LoRA training of Google's Gemma 3-4B instruction-tuned model. It configures 4-bit quantization, LoRA with rank-16 adaptation targeting attention and MLP layers (q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj), and applies supervised fine-tuning with 100 training steps at 2e-4 learning rate. The notebook produces two fine-tuned model variants: `gemma-3-4b-factual` (optimized for pure factual recall) and `gemma-3-4b-abstain` (trained to output "I don't know" when uncertain).

Evaluation results (`evaluate.py` and `evaluate_abstrain.py`) measure both local inference performance and abstention correctness across the 17,726-question test set. The factual model achieved 8.5% accuracy (1,499 correct) while the abstention model achieved 8.6% accuracy (1,527 correct), demonstrating that explicit abstention training provides marginal improvement over factual-only training but both substantially underperform the baseline Gemini 2.5 Flash Lite at 50.1%. This confirms that architectural enforcement—via knowledge graph licensing—is necessary for principled abstention, as parameter-optimized prompting alone cannot reliably prevent hallucination across the vast, underdetermined factual space.

