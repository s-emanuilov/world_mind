# Evaluation Summary: Baseline LLM Performance on River Knowledge

The `evaluation/` directory contains experimental results assessing baseline LLM performance on factual knowledge tasks using the river Q&A dataset. Three models were evaluated: Anthropic's Claude Sonnet 4.5, Google's Gemini 2.5 Flash Lite, and Google's Gemma 3-4B instruction-tuned variant. These benchmarks establish the hallucination baseline that the World Mind architecture aims to systematically address through knowledge-graph-licensed abstention.

Claude Sonnet 4.5 achieved 42.0% accuracy on 4,208 questions tested, demonstrating that even high-parameter models struggle significantly with domain-specific factual knowledge outside their training distribution. Gemini 2.5 Flash Lite, a mid-range model optimized for speed, achieved 50.1% accuracy on 12,174 questionsâ€”slightly above chance for 5-option multiple-choice questions but far from the reliability threshold required for grounded generation systems. Gemma 3-4B, representing compact open-source instruction models, scored only 16.7% accuracy on 7,839 questions, clearly showing the inadequacy of parameter-count scaling alone for factual precision.

The evaluation infrastructure (`evaluate_llms.py`) implements a resumable, incremental assessment pipeline that queries models via OpenRouter API, extracts single-character responses (A-E), and tracks correctness against ground truth. Results are serialized as JSONL records containing question metadata, model responses, correctness flags, and timestamps, enabling detailed failure mode analysis. The system demonstrates baseline performance patterns that validate the research hypothesis: current LLMs fundamentally lack the architectural capacity to withhold when evidence is absent, instead producing plausible-sounding but ungrounded assertions.

These baseline measurements provide the empirical foundation for comparative evaluation against the proposed graph-licensed abstention system, directly supporting the research contribution of demonstrating how architectural enforcement of truth constraints can systematically improve reliability over probabilistic prompt engineering alone.

