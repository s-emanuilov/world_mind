# World Mind POC-4 Rivers Extended - Paper Documentation

This directory contains the complete academic write-up, figures, and supporting documentation for the experimental validation of truth-constrained generation via graph-licensed abstention.

---

## ğŸ“„ Main Documents

### 1. `experiment_summary.md` (Primary Paper)
**The complete academic paper** describing methodology, results, and implications.

**Contents**:
- Introduction and theoretical framing
- Dataset construction (9,538 rivers, 17,726 questions)
- Five experimental approaches (baseline â†’ fine-tuning â†’ RAG â†’ Graph-RAG)
- Comprehensive results and statistical analysis
- Discussion of architectural vs. statistical solutions
- Reproducibility section with artifact links

**Length**: ~8,000 words  
**Format**: Academic paper suitable for journal submission  
**Audience**: Researchers in AI, NLP, knowledge graphs

**Figures referenced**: 5 publication-quality visualizations (see `graphs.md`)

---

### 2. `graphs.md` (Visualization Code)
**Python code for generating all figures** with publication-quality styling.

**Figures**:
1. **Baseline Performance** - Three models without grounding (16.7-50.1% accuracy)
2. **Fine-Tuning Results** - Failed parameter optimization + abstention analysis
3. **RAG Comparison** - Embedding vs. Graph-RAG (~89% both, different capabilities)
4. **Complete Progression** - Journey from statistical to architectural approaches
5. **Capabilities Matrix** - Feature comparison across all systems

**Technology**: matplotlib, seaborn, numpy  
**Output**: PNG (300 DPI) + PDF (vector) formats  
**Style**: Academic publication-ready with consistent theming

**Run**:
```bash
python graphs.md  # or execute in Jupyter
```

---

### 3. `QUICK_REFERENCE.md` (Executive Summary)
**Condensed 2-page summary** of key findings and methodology.

**Best for**:
- Quick overview of results
- Conference presentations
- Grant proposals
- Initial paper reviews

**Includes**:
- Core thesis statement
- Results table (all 8 evaluations)
- Critical insights (3 main findings)
- Licensing oracle paradigm explanation
- One-sentence summary

---

### 4. `ARTIFACTS_SUMMARY.md` (Release Checklist)
**Complete inventory of public artifacts** for reproducibility.

**Sections**:
- Datasets (4 HuggingFace repositories)
- Fine-tuned models (2 LoRA variants)
- Evaluation results (111K+ records)
- Code repository structure
- Citation information
- Release checklist

**HuggingFace username**: `s-emanuilov`  
**GitHub repository**: `s-emanuilov/world-mind-poc-rivers`

---

## ğŸ“‹ Supporting Documents (from Experimental Process)

### Planning and Feedback
- `plan.md` - Original research plan and methodology design
- `feedback.md` - Consolidated feedback on experimental approach
- `feedback_datasets.md` - Dataset design validation
- `claude_chellenge.md` - Critical analysis of architectural vs. statistical claims
- `chatgpt_feedback.md` - Prompt engineering and evaluation strategy

These documents capture the iterative refinement process and critical challenges addressed during experimental design.

---

## ğŸ¯ Quick Navigation

### If you want to...

**â†’ Understand the complete experimental work**  
Read: `experiment_summary.md` (start here)

**â†’ See visual results quickly**  
Generate: `graphs.md` â†’ produces 5 figures

**â†’ Get a fast overview for a presentation**  
Read: `QUICK_REFERENCE.md` (2 pages)

**â†’ Reproduce the experiments**  
Check: `ARTIFACTS_SUMMARY.md` â†’ links to all code and data

**â†’ Understand design decisions**  
Review: Planning documents (`plan.md`, `feedback*.md`)

**â†’ Prepare for publication**  
Use: `experiment_summary.md` + `graphs.md` + `ARTIFACTS_SUMMARY.md`

---

## ğŸ“Š Key Results at a Glance

| System | Accuracy | Key Characteristic |
|--------|----------|-------------------|
| Baseline LLMs | 16.7-50.1% | No grounding mechanism |
| Fine-tuned Models | 8.5-8.6% | **Degradation** despite supervision |
| Embedding RAG | **89.5%** | Statistical retrieval |
| Graph-RAG Oracle | **89.1%** | Architectural enforcement |

**Main finding**: Context provision (RAG) dominates factual accuracy, but architectural enforcement (Graph-RAG) uniquely enables deterministic abstention and formal validation.

---

## ğŸ”¬ Experimental Progression

```
Step 1: SPARQL Data Extraction
   â†“ (9,538 rivers from DBpedia)

Step 2: LLM-Augmented Enhancement  
   â†“ (Fill missing values)

Step 3: Question Generation
   â†“ (17,726 Q&A pairs)

Step 4: Baseline Evaluation
   â†“ (3 models: 16.7-50.1%)

Step 5: Fine-Tuning Experiments
   â†“ (2 variants: both ~8.5%, failed)

Step 6: Embedding-based RAG
   â†“ (89.5%, strong performance)

Step 7: Graph Construction
   â†“ (118K triples + SHACL constraints)

Step 8: Graph-RAG with Oracle
   â†“ (89.1%, architectural guarantees)

Step 9: Comparative Analysis
   â†’ Paper + Figures + Public Release
```

---

## ğŸ† Research Contribution

**Thesis**: Hallucination is an **architectural limitation**, not a statistical deficiency.

**Evidence**:
1. Fine-tuning 17K examples **degraded** performance
2. Abstention training achieved only 56.7% precision
3. RAG provides 39pp improvement via context provision
4. Graph-RAG maintains accuracy while adding formal guarantees

**Innovation**: **Licensing oracle** architecture where knowledge graph gates generation through mandatory SHACL validationâ€”moving from retrieval-augmented generation to constraint-governed generation.

---

## ğŸ“¦ File Manifest

```
paper/
â”œâ”€â”€ README.md                      # This file - navigation guide
â”œâ”€â”€ experiment_summary.md          # Main academic paper (~8K words)
â”œâ”€â”€ graphs.md                      # Figure generation code (Python)
â”œâ”€â”€ QUICK_REFERENCE.md             # 2-page executive summary
â”œâ”€â”€ ARTIFACTS_SUMMARY.md           # Public release inventory
â”œâ”€â”€ plan.md                        # Original research plan
â”œâ”€â”€ feedback.md                    # Consolidated feedback
â”œâ”€â”€ feedback_datasets.md           # Dataset design notes
â”œâ”€â”€ claude_chellenge.md            # Critical analysis (18KB)
â””â”€â”€ chatgpt_feedback.md           # Evaluation strategy notes
```

**Generated outputs** (after running `graphs.md`):
```
â”œâ”€â”€ figure_1_baseline_performance.png / .pdf
â”œâ”€â”€ figure_2_finetuning_results.png / .pdf
â”œâ”€â”€ figure_3_rag_comparison.png / .pdf
â”œâ”€â”€ figure_4_complete_progression.png / .pdf
â””â”€â”€ figure_5_capabilities_matrix.png / .pdf
```

---

## ğŸš€ Getting Started

### For Readers
1. Start with `QUICK_REFERENCE.md` for overview
2. Read `experiment_summary.md` for complete details
3. Generate figures with `graphs.md` for visual understanding

### For Reproducers
1. Check `ARTIFACTS_SUMMARY.md` for all data/code links
2. Clone GitHub repository: `s-emanuilov/world-mind-poc-rivers`
3. Follow README instructions for setup and execution

### For Reviewers
1. Read `experiment_summary.md` (methodology + results)
2. Review figures (generate via `graphs.md`)
3. Check planning documents for experimental rigor
4. Verify reproducibility via `ARTIFACTS_SUMMARY.md`

---

## ğŸ“š Citation

```bibtex
@article{emanuilov2025truthconstrained,
  title={Experimental Validation of Truth-Constrained Generation 
         via Graph-Licensed Abstention},
  author={Emanuilov, Simeon and [Co-authors]},
  journal={[Journal/Conference TBD]},
  year={2025},
  note={Code and data: github.com/s-emanuilov/world-mind-poc-rivers; 
        Models: huggingface.co/s-emanuilov}
}
```

---

## ğŸ“§ Contact

**Author**: Simeon Emanuilov  
**HuggingFace**: [@s-emanuilov](https://huggingface.co/s-emanuilov)  
**GitHub**: [@s-emanuilov](https://github.com/s-emanuilov)

For questions about methodology, results, or artifacts, please open an issue on the GitHub repository.

---

## âœ… Publication Readiness

- [x] Complete experimental methodology documented
- [x] All results verified against raw data
- [x] 5 publication-quality figures with generation code
- [x] Reproducibility artifacts identified and documented
- [x] Executive summary for quick reference
- [x] Critical analysis and limitations discussed
- [x] Consistent academic tone and rigor throughout
- [ ] Final review by co-authors
- [ ] Figures generated and quality-checked
- [ ] Datasets uploaded to HuggingFace
- [ ] Models uploaded to HuggingFace
- [ ] Code repository created on GitHub
- [ ] Journal/conference selected for submission

---

**Status**: Documentation complete, ready for artifact preparation and submission  
**Version**: 1.0  
**Last Updated**: October 28, 2025


