# RAG Experiment Summary: Embedding-Based Retrieval Baseline

The `rag_experiment/` directory implements a standard embedding-based Retrieval-Augmented Generation system using the `intfloat/multilingual-e5-large-instruct` model to provide a performance baseline for comparison against the Graph-RAG architecture. This system processes river abstracts into semantic chunks, generates 1024-dimensional embeddings, retrieves context via cosine similarity search, and augments LLM prompts with retrieved passages—representing the conventional RAG paradigm that treats knowledge as unstructured text rather than structured ontological relationships.

**Embedding Pipeline:** The system extracts and chunks river information from `raw_rivers_filled.csv` into 400-character segments with 50-character overlap, preserving metadata (river names, geographic attributes, hydrological metrics, administrative details). `generate_embeddings.py` applies the multilingual-e5-large-instruct model with instruction-based queries ("Given a question about US rivers...") to produce contextual embeddings optimized for retrieval tasks. The embedding approach treats each river's abstract as a semantic vector space where proximity indicates conceptual similarity—fundamentally different from the explicit relational structure of graph-based methods.

**Retrieval Architecture:** `retrieval_system.py` implements cosine similarity search across the embedding space, returning top-k document chunks (default k=5) with similarity scores above 0.7 threshold. For each question, the system computes embedding similarity, ranks passages, and injects the most relevant context into the LLM prompt. This approach relies on distributional semantics—questions and answers are matched indirectly through learned representations rather than direct attribute matching or logical entailment.

**Evaluation Results:** The RAG system achieved approximately 89.57% accuracy (13,167 correct / 14,700 questions), statistically equivalent to the Graph-RAG system's 89.1% performance. This near-identical accuracy demonstrates that, for factual retrieval tasks, embedding-based and graph-based RAG can produce comparable results when both augment LLMs with relevant contextual information. However, the architectural difference becomes critical when the system must abstain or validate claims beyond pure retrieval.

**Critical Distinction:** While both systems achieve similar accuracy, the Graph-RAG approach's structured knowledge representation enables principled validation and abstention capabilities that embedding-based RAG cannot support. The graph's ontological constraints, SHACL validation, and explicit relationships provide architectural mechanisms for claiming confidence or deferring when evidence is insufficient. Embedding-based RAG, by contrast, has no structural basis for epistemological boundaries—it cannot distinguish between high-confidence factual retrievals and low-confidence similarity matches, making it unsuitable for implementing the World Mind licensing oracle that enforces principled abstention.

